pacman::p_load(rvest)

##Inicialization
climabog<-list()
climabog_data<-list()

for (i in 1:12){
#Specifying the url for desired website to be scrapped
url <- paste0('https://www.tutiempo.net/clima/',ifelse(i<10,paste0(0,i),i),'-2014/ws-802220.html')
#Reading the HTML code from the website
webpage <- read_html(url)

#Using CSS selectors to scrap the stores names and addresses 
climabog <- html_nodes(webpage,'.mensuales')


#Converting the html data to text
climabog_data[[i]]<- html_table(climabog)

}

## Converting list to dataframe
pacman::p_load(stringi,data.table)
Res<-matrix(NA,396,33)

month<-list()
for(i in 1:12){
  for(j in 1:length(climabog_data[[i]])){
    if(is.null(climabog_data[[i]][[j]])==FALSE){
      Res <- data.frame(stri_list2matrix(climabog_data[[i]][[j]]))
    }}
  month[[i]]<-rbind(Res)
  Res<-list()
}

Res2<-matrix(NA,396,33)
clima<-as.data.frame(matrix(0,0,15))
colnames(clima)<-c('X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12',
                   'X13','X14','X15')
for(i in 1:length(month)){
 
    Res2 <- data.frame(stri_list2matrix(month[[i]]))
    
  
  clima<-rbind(clima,Res2)
  }

colnames(clima)<-c('DiaMes','T_Med','T_Max','T_Min','Pres_Atm','Hum_Rel_Med','Prec_Tot',
                   'Vis_Med','Vel_Med_Viento','Vel_Max_S_Viento','Vel_Max_Raf_Viento',
                   'Ind_Lluvia','Ind_Nieve','Ind_Tormenta','Ind_Niebla')


## Cleaning data


delete<-which(stri_length(clima$DiaMes)>2|stri_length(clima$DiaMes)==0)
clima<-clima[-delete,]
for(i in 1:11){clima[,i]<-as.numeric(as.character(clima[,i]))}
clima$Ind_Lluvia<-ifelse(clima$Ind_Lluvia=="o",1,0)
clima$Ind_Nieve<-ifelse(clima$Ind_Nieve=="o",1,0)
clima$Ind_Tormenta<-ifelse(clima$Ind_Tormenta=="o",1,0)
clima$Ind_Niebla<-ifelse(clima$Ind_Niebla=="o",1,0)

clima$Fecha<-as.Date(seq(1,365,1),origin = '2013-12-31')

datos<-read.csv("base.csv")
datos$Fecha<-as.Date(as.character(datos$Fecha),"%d/%m/%Y")
str(datos)

pacman::p_load(dplyr)
datos<-left_join(datos,clima,by="Fecha")

write.csv(datos,"base.csv",row.names = F)
